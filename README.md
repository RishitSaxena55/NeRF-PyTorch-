<div align="center">

# ğŸŒ NeRF: Neural Radiance Fields from Scratch

**A from-scratch PyTorch implementation of Neural Radiance Fields for novel view synthesis**

[![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)](https://pytorch.org)
[![License](https://img.shields.io/badge/License-Apache_2.0-blue?style=for-the-badge)](LICENSE)

[Paper](https://www.matthewtancik.com/nerf) â€¢ [Architecture](#-architecture) â€¢ [Components](#-core-components) â€¢ [Results](#-results)

</div>

---

> ğŸ¯ **A modular, from-scratch implementation** of Neural Radiance Fields designed for research experimentation and transparent understanding of differentiable volume rendering.

<div align="center">

![NeRF Pipeline](imgs/pipeline.jpg)

</div>

---

## ğŸ“Œ About This Project

This repository contains a **modular from-scratch implementation** of Neural Radiance Fields (NeRF). Each component is implemented independently to enable research experimentation and easy modification:

- âœ… **Positional Encoding** â€” Fourier feature mapping for high-frequency detail
- âœ… **MLP Architecture** â€” The neural radiance field network
- âœ… **Ray Generation** â€” Camera ray casting and sampling
- âœ… **Hierarchical Sampling** â€” Coarse-to-fine importance sampling
- âœ… **Volume Rendering** â€” Differentiable alpha compositing

The `nerf-pytorch/` folder contains a reference implementation for comparison and demo purposes.

---

## ğŸ—ï¸ Architecture

```
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚         Input: Ray (o, d)           â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚     Sample Points Along Ray         â”‚
                        â”‚         tâ‚, tâ‚‚, ..., tâ‚™             â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                                â”‚                                â”‚
         â–¼                                â–¼                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Positional     â”‚              â”‚  Positional     â”‚              â”‚  Positional     â”‚
â”‚  Encoding Î³(x)  â”‚              â”‚  Encoding Î³(x)  â”‚              â”‚  Encoding Î³(x)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                â”‚                                â”‚
         â–¼                                â–¼                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MLP Network   â”‚              â”‚   MLP Network   â”‚              â”‚   MLP Network   â”‚
â”‚  F: (x,d)â†’(c,Ïƒ) â”‚              â”‚  F: (x,d)â†’(c,Ïƒ) â”‚              â”‚  F: (x,d)â†’(c,Ïƒ) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                â”‚                                â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚       Volume Rendering              â”‚
                        â”‚   C(r) = Î£ Táµ¢(1-exp(-Ïƒáµ¢Î´áµ¢))cáµ¢      â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â”‚
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚        Output: RGB Pixel            â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§© Core Components

### 1. Positional Encoding (`positional_encoding.py`)

Maps low-dimensional inputs to high-dimensional space using Fourier features, enabling the network to learn high-frequency scene details.

```python
Î³(p) = [sin(2â°Ï€p), cos(2â°Ï€p), sin(2Â¹Ï€p), cos(2Â¹Ï€p), ..., sin(2^(L-1)Ï€p), cos(2^(L-1)Ï€p)]
```

**Key Implementation Details:**
- Position encoding with L=10 frequencies (60D output)
- Direction encoding with L=4 frequencies (24D output)
- Enables learning of fine geometric structures and textures

---

### 2. NeRF MLP Network (`model.py`)

The core neural network that maps 3D position + viewing direction to RGB color and volume density.

```python
class NeRF(nn.Module):
    """
    Architecture:
    - 8 fully-connected layers (256 units each) with ReLU
    - Skip connection at layer 5 (concatenates input)
    - Density Ïƒ output after layer 8
    - Additional layer for view-dependent color
    """
```

**Network Design:**
- Input: Encoded position (60D) + Encoded direction (24D)
- 8 dense layers with skip connection at layer 5
- Output: RGB (3D) + Density Ïƒ (1D)

---

### 3. Ray Helpers (`ray_helpers.py`)

Utilities for generating camera rays from pixel coordinates.

```python
def get_rays(H, W, focal, c2w):
    """
    Generate rays for each pixel in the image.
    
    Returns:
        rays_o: Ray origins (H, W, 3)
        rays_d: Ray directions (H, W, 3)
    """
```

**Implemented Functions:**
- Camera intrinsic matrix handling
- World-to-camera transformations
- Ray origin and direction computation

---

### 4. Hierarchical Sampling (`hierarchical_sampling.py`)

Two-stage sampling strategy for efficient rendering:

```python
# Coarse sampling: Uniform samples along ray
t_coarse = stratified_sampling(t_near, t_far, N_coarse)

# Fine sampling: Importance sampling based on coarse weights
t_fine = importance_sampling(t_coarse, weights, N_fine)
```

**Benefits:**
- Concentrates samples in regions that contribute most to the final color
- Reduces computational waste in empty space
- Improves rendering quality with same sample budget

---

### 5. Volume Rendering (`volume_rendering.py`)

The differentiable rendering equation that converts density and color to pixel values.

```python
def volume_render(rgb, density, t_vals, rays_d):
    """
    Classic volume rendering with alpha compositing.
    
    C(r) = Î£áµ¢ Táµ¢ Â· (1 - exp(-Ïƒáµ¢ Â· Î´áµ¢)) Â· cáµ¢
    
    where:
        Táµ¢ = exp(-Î£â±¼â‚Œâ‚â±â»Â¹ Ïƒâ±¼ Â· Î´â±¼)  (transmittance)
        Î´áµ¢ = táµ¢â‚Šâ‚ - táµ¢              (distance between samples)
    """
```

**Key Features:**
- Fully differentiable for end-to-end training
- Computes depth maps as byproduct
- Handles view-dependent effects (reflections, specularity)

---

## ğŸ“ Project Structure

```
NeRF-PyTorch-/
â”œâ”€â”€ model.py                 # NeRF MLP architecture
â”œâ”€â”€ positional_encoding.py   # Fourier feature encoding
â”œâ”€â”€ ray_helpers.py           # Camera ray generation
â”œâ”€â”€ hierarchical_sampling.py # Coarse-to-fine sampling
â”œâ”€â”€ volume_rendering.py      # Differentiable rendering
â”œâ”€â”€ requirements.txt         # Dependencies
â”œâ”€â”€ nerf-pytorch/            # Reference implementation (for demo)
â””â”€â”€ imgs/                    # Sample results
```

---

## ğŸ¨ Capabilities

**Supported Datasets:**
| Dataset | Scenes | Resolution |
|---------|--------|------------|
| **NeRF Synthetic** | Lego, Chair, Drums, Ficus, Hotdog, Materials, Mic, Ship | 800Ã—800 |
| **LLFF** | Fern, Flower, Fortress, Horns, Leaves, Orchids, Room, Trex | 1008Ã—756 |

**Rendered Outputs:**
- ğŸ–¼ï¸ Novel view RGB images
- ğŸ“ Depth maps  
- ğŸ¥ 360Â° video synthesis

---

## ğŸ”¬ Technical Insights

### Why Positional Encoding?
Neural networks are biased towards learning low-frequency functions. By mapping inputs to a higher-dimensional space using Fourier features, we enable the network to capture high-frequency variations in geometry and appearance.

### Why Hierarchical Sampling?
Naive uniform sampling wastes computation in empty regions. By first doing coarse sampling to identify important regions, then focusing samples there, we achieve better quality with fewer total samples.

### Why View-Dependent Color?
Real materials exhibit view-dependent appearance (specular highlights, reflections). By conditioning the color output on viewing direction, NeRF can represent these non-Lambertian effects.

---

## ğŸš€ Getting Started

```bash
# Clone the repository
git clone https://github.com/RishitSaxena55/NeRF-PyTorch-.git
cd NeRF-PyTorch-

# Install dependencies
pip install -r requirements.txt

# For demo with reference implementation
cd nerf-pytorch
python run_nerf.py --config configs/lego.txt
```

---

## ğŸ“š References

- [NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis](https://www.matthewtancik.com/nerf) (Mildenhall et al., ECCV 2020)
- [Fourier Features Let Networks Learn High Frequency Functions](https://bmild.github.io/fourfeat/) (Tancik et al., NeurIPS 2020)

---

## ğŸ¤ Connect

Built with ğŸ’œ by [Rishit Saxena](https://github.com/RishitSaxena55)

[![Portfolio](https://img.shields.io/badge/Portfolio-rishitsaxena55.github.io-8B5CF6?style=flat-square)](https://rishitsaxena55.github.io)
[![Email](https://img.shields.io/badge/Email-rishitsaxena55@gmail.com-EA4335?style=flat-square)](mailto:rishitsaxena55@gmail.com)

---

## ğŸ“ License

This project is licensed under the Apache 2.0 License - see the [LICENSE](LICENSE) file for details.
